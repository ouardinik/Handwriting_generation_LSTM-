{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils.gmm import mix_coef,gmm2d,gmm_loss\n",
    "from utils.model import lstm_model\n",
    "from utils.batch_generator import batch_generator, dataset_generator\n",
    "import scipy.stats\n",
    "import copy\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "batch_size = 5\n",
    "seq_length_model = 600\n",
    "n_units = 900\n",
    "lr = 0.001\n",
    "N_mixtures = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################ MODEL #############################################################\n",
    "tf.reset_default_graph()\n",
    "######################### define contants #####################################\n",
    "batch_size = 50\n",
    "seq_length = 600\n",
    "# number of units of the hidden layer\n",
    "n_units = 900\n",
    "# learning rate for \"\"\n",
    "lr = 0.001\n",
    "# parameters for gmm\n",
    "N_mixtures = 20\n",
    "tot_mixtures = N_mixtures*6 + 1\n",
    "\n",
    "#weights and biases of appropriate shape to accomplish above task\n",
    "out_weights = tf.get_variable(\"w_y\", [n_units, tot_mixtures])\n",
    "out_bias = tf.get_variable(\"b_y\", [tot_mixtures])\n",
    "\n",
    "########################### Define placeholders #######################################\n",
    "#input batch of strokes placeholder\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None,seq_length,3])\n",
    "#input label placeholder\n",
    "targets = tf.placeholder(dtype=tf.float32, shape=[None,seq_length,3])\n",
    "sample_stroke = tf.placeholder(dtype=tf.float32, shape=[1,1,3])\n",
    "\n",
    "########################## Define network ############################################\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=n_units,state_is_tuple=True)\n",
    "init_state = cell.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "init_sample = cell.zero_state(batch_size=1, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# reshape target data\n",
    "flat_targets = tf.reshape(targets, [-1, 3])\n",
    "# get position values (x,y) and end of stroke data\n",
    "split_e, split_x, split_y = tf.split(value=flat_targets, axis=1, num_or_size_splits=3)\n",
    "    \n",
    "\n",
    "#split in a list of T time steps where T is the sequence length\n",
    "#batch_X = tf.unstack(X, axis=1)            \n",
    "split_outputs, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=X,\n",
    "                                   initial_state=init_state,\n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "#For sampling\n",
    "output_sample, out_sample = tf.nn.dynamic_rnn(cell=cell, inputs=sample_stroke,\n",
    "                                   initial_state=init_sample,\n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "flat_outputs = tf.reshape(split_outputs,[-1,n_units])\n",
    "output = tf.matmul(flat_outputs,out_weights) + out_bias\n",
    "#For sampling:\n",
    "flat_outputs_sample = tf.reshape(output_sample,[-1,n_units])\n",
    "output_sample = tf.matmul(flat_outputs_sample,out_weights) + out_bias\n",
    "\n",
    "################################## Using utils.gmm functions ################################\n",
    "#get mixture gmm coeff:\n",
    "op_pi, op_mu_x, op_mu_y, op_std_x, op_std_y, op_rho, op_param_e  = mix_coef(output)\n",
    "# For sampling\n",
    "sk_pi, sk_mu_x, sk_mu_y, sk_std_x, sk_std_y, sk_rho, sk_param_e  = mix_coef(output_sample)\n",
    "\n",
    "#compute loss:\n",
    "op_loss = gmm_loss(split_e, split_x, split_y,op_pi, op_mu_x, op_mu_y, op_std_x, op_std_y, op_rho, op_param_e )\n",
    "total_loss = op_loss/(batch_size*seq_length) \n",
    "\n",
    "##################################### TRAINING ######################################\n",
    "parameters = tf.trainable_variables()\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=lr)\n",
    "grds = optimizer.compute_gradients(total_loss)\n",
    "LSTM_grds = [grds[2],grds[3]]\n",
    "out_grds = [grds[0],grds[1]]\n",
    "clipped_grds = [(tf.clip_by_value(grad1, -100., 100.), var1) for grad1, var1 in out_grds]+[(tf.clip_by_value(grad2, -10., 10.),\n",
    "                                                                                            var2) for grad2, var2 in LSTM_grds]               \n",
    "train_op = optimizer.apply_gradients(clipped_grds)\n",
    "#train_op = optimizer.minimize(total_loss)\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "#ops that we will restore for sampling\n",
    "#sample_stroke = tf.identity(sample_stroke,name=\"op_to_restore\")  \n",
    "out_sample = tf.identity(out_sample, name=\"op_to_restore\")\n",
    "sk_pi = tf.identity(sk_pi, name=\"op_to_restore\")\n",
    "sk_mu_x = tf.identity(sk_mu_x, name=\"op_to_restore\")\n",
    "sk_mu_y = tf.identity(sk_mu_y, name=\"op_to_restore\")\n",
    "sk_std_x = tf.identity(sk_std_x, name=\"op_to_restore\")\n",
    "sk_std_y = tf.identity(sk_std_y, name=\"op_to_restore\")\n",
    "sk_rho = tf.identity(sk_rho, name=\"op_to_restore\")\n",
    "sk_param_e = tf.identity(sk_param_e, name=\"op_to_restore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################# prepare data for training ##########################################\n",
    "seq_length = 700\n",
    "inputs = np.load('data/strokes.npy',encoding='bytes')\n",
    "all_inputs = dataset_generator(inputs,seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training and validation set\n",
    "X_train = np.copy(all_inputs[0:5000])\n",
    "X_val = np.copy(all_inputs[5000:6000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "num_batches = int(X_train.shape[0]/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_train_losses = []\n",
    "all_valid_losses = []\n",
    "time_epochs = []\n",
    "time_batches = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################################## TRAINING ####################################\n",
    "sess = tf.Session() \n",
    "sess.run(init_op)\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "for epoch in range(num_epochs):\n",
    "    start_e = time.time()\n",
    "    valid_batch, valid_labels = batch_generator(X_val,batch_size,600)\n",
    "    valid_feed = { X:valid_batch, targets:valid_labels }\n",
    "    np.random.shuffle(X_train)\n",
    "    for btch in range(num_batches):\n",
    "        train_batch,labels_batch = batch_generator(X_train,batch_size,600)\n",
    "        feed_dict = {X:train_batch, targets:labels_batch}\n",
    "        start_b = time.time()\n",
    "        train_loss, out_state, _ = sess.run([total_loss,\n",
    "                                                final_state,\n",
    "                                                train_op], \n",
    "                                                feed_dict)\n",
    "        print(train_loss)                                                             \n",
    "        all_train_losses.append(train_loss)                                                           \n",
    "            \n",
    "        valid_loss = sess.run([total_loss], valid_feed)\n",
    "        all_valid_losses.append(valid_loss)                                         \n",
    "        #saver.save(sess, './LSTM_900_2xclip.chkp')\n",
    "        end_b = time.time()\n",
    "        time_batches.append(end_b-start_b)   \n",
    "        end_e = time.time()\n",
    "        time_epochs.append(end_e-start_e)\n",
    "    saver.save(sess, './LSTM_900_2xclip.chkp') \n",
    "    print('end of epoch number',epoch,'valid loss is',valid_loss)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desired_seq_length = 600\n",
    "#sess = tf.Session()\n",
    "sample = np.zeros((desired_seq_length,3))\n",
    "current_stroke = np.zeros((1,1,3))\n",
    "previous_stroke = sess.run(cell.zero_state(batch_size=1,dtype=tf.float32))\n",
    "for k in range(desired_seq_length):\n",
    "    feed_dict = {sample_stroke:current_stroke, init_sample:previous_stroke}\n",
    "    [pi0, mu1, mu2, std1, std2, rho0, param_e0,next_stroke] = sess.run([sk_pi, sk_mu_x, sk_mu_y,\n",
    "                                                                        sk_std_x, sk_std_y,\n",
    "                                                                        sk_rho, sk_param_e, out_sample],\n",
    "                                                                        feed_dict)\n",
    "    #pick a random mixture\n",
    "    mix = np.random.randint(0,20)\n",
    "    mu = np.array([mu1[0,mix],mu2[0,mix]])\n",
    "    sigma = np.array([[std1[0,mix]*std1[0,mix],rho0[0,mix]*std1[0,mix]*std2[0,mix]],\n",
    "                      [rho0[0,mix]*std1[0,mix]*std2[0,mix],std2[0,mix]*std2[0,mix]]])\n",
    "    #generate data\n",
    "    z = np.random.multivariate_normal(mu, sigma, 1)\n",
    "    z_e = scipy.stats.bernoulli.rvs(param_e0, size=1)\n",
    "    sample[k] = [z_e, z[0,0], z[0,1]]\n",
    "    \n",
    "    current_stroke = np.zeros((1,1,3))\n",
    "    current_stroke[0][0] = [z_e, z[0,0], z[0,1]]\n",
    "    previous_stroke = next_stroke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
